{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1831d98-e02b-4e23-8927-1dd5cee16839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This paper introduces a Projected Principal Component Analysis (Projected-PCA), which employees principal component\\n   analysis to the projected (smoothed) data matrix onto a given linear space spanned by covariates. When it applies to \\n   high-dimensional factor analysis, the projection removes noise components.\\n   PCA can be used for projecting and visualizing data in lower dimensions. Explanation: Sometimes it is very useful to\\n   plot the data in lower dimensions. We can take the first 2 principal components and then use visualization of the data\\n   using a scatter plot.\\n   The last step of PCA is we need to multiply Q tranpose of Q with the original data matrix in order to get the \\n   projection matrix. We go from the (d x k) Q matrix and Q transpose of Q results in d x d dimension. By multiplying\\n   the (d x n) X matrix, the projection matrix is d x n.\\n   '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q1\n",
    "'''This paper introduces a Projected Principal Component Analysis (Projected-PCA), which employees principal component\n",
    "   analysis to the projected (smoothed) data matrix onto a given linear space spanned by covariates. When it applies to \n",
    "   high-dimensional factor analysis, the projection removes noise components.\n",
    "   PCA can be used for projecting and visualizing data in lower dimensions. Explanation: Sometimes it is very useful to\n",
    "   plot the data in lower dimensions. We can take the first 2 principal components and then use visualization of the data\n",
    "   using a scatter plot.\n",
    "   The last step of PCA is we need to multiply Q tranpose of Q with the original data matrix in order to get the \n",
    "   projection matrix. We go from the (d x k) Q matrix and Q transpose of Q results in d x d dimension. By multiplying\n",
    "   the (d x n) X matrix, the projection matrix is d x n.\n",
    "   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95b01899-c7b0-484c-99c5-a0e0ce82abea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PCA seeks to solve a sequence of optimization problems. The first in the sequence is the unconstrained problem\\n   maximizeuTSuuTu,u∈Rp. Since uTu=‖u‖22=‖u‖‖u‖, the above unconstrained problem is equivalent to the constrained problem\\n   maximizeuTSusubject touTu=1.\\n   PCA is a tool for identifying the main axes of variance within a data set and allows for easy data exploration to\\n   understand the key variables in the data and spot outliers. Properly applied, it is one of the most powerful tools in \\n   the data analysis tool kit.\\n   PCA is a tool for identifying the main axes of variance within a data set and allows for easy data exploration to \\n   understand the key variables in the data and spot outliers. Properly applied, it is one of the most powerful tools in\\n   the data analysis tool kit.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q2\n",
    "'''PCA seeks to solve a sequence of optimization problems. The first in the sequence is the unconstrained problem\n",
    "   maximizeuTSuuTu,u∈Rp. Since uTu=‖u‖22=‖u‖‖u‖, the above unconstrained problem is equivalent to the constrained problem\n",
    "   maximizeuTSusubject touTu=1.\n",
    "   PCA is a tool for identifying the main axes of variance within a data set and allows for easy data exploration to\n",
    "   understand the key variables in the data and spot outliers. Properly applied, it is one of the most powerful tools in \n",
    "   the data analysis tool kit.\n",
    "   PCA is a tool for identifying the main axes of variance within a data set and allows for easy data exploration to \n",
    "   understand the key variables in the data and spot outliers. Properly applied, it is one of the most powerful tools in\n",
    "   the data analysis tool kit.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "195dac29-a749-4b8c-a441-81430d0d5d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Covariance-based PCA is equivalent to MLPCA whenever the variance-covariance matrix of the measurement errors is\\n   assumed diagonal with equal elements on its diagonal. The measurement error variance parameter can then be estimated \\n   by applying the probabilistic principal component analysis (PPCA) model [5].\\n   PCA is simply described as \"diagonalizing the covariance matrix\". It simply means that we need to find a non-trivial \\n   linear combination of our original variables such that the covariance matrix is diagonal.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q3\n",
    "'''Covariance-based PCA is equivalent to MLPCA whenever the variance-covariance matrix of the measurement errors is\n",
    "   assumed diagonal with equal elements on its diagonal. The measurement error variance parameter can then be estimated \n",
    "   by applying the probabilistic principal component analysis (PPCA) model [5].\n",
    "   PCA is simply described as \"diagonalizing the covariance matrix\". It simply means that we need to find a non-trivial \n",
    "   linear combination of our original variables such that the covariance matrix is diagonal.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a591c36-b46a-4798-942b-5a33d2592183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Principal component analysis (PCA) simplifies the complexity in high-dimensional data while retaining trends and\\n   patterns. It does this by transforming the data into fewer dimensions, which act as summaries of features.\\n   Each column of rotation matrix contains the principal component loading vector. This is the most important measure\\n   we should be interested in. This returns 44 principal component loadings.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q4\n",
    "'''Principal component analysis (PCA) simplifies the complexity in high-dimensional data while retaining trends and\n",
    "   patterns. It does this by transforming the data into fewer dimensions, which act as summaries of features.\n",
    "   Each column of rotation matrix contains the principal component loading vector. This is the most important measure\n",
    "   we should be interested in. This returns 44 principal component loadings.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "058c1bc2-e312-4afb-b3c2-6524480a3550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Feature extraction: PCA can be used to identify the most important features in a dataset, which can be used to build\\n   predictive models. Visualization: PCA can be used to visualize high-dimensional data in two or three dimensions,\\n   making it easier to understand and interpret.\\n   A feature selection method is proposed to select a subset of variables in principal component analysis (PCA) that\\n   preserves as much information present in the complete data as possible. The information is measured by means of the\\n   percentage of consensus in generalised Procrustes analysis.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q5\n",
    "'''Feature extraction: PCA can be used to identify the most important features in a dataset, which can be used to build\n",
    "   predictive models. Visualization: PCA can be used to visualize high-dimensional data in two or three dimensions,\n",
    "   making it easier to understand and interpret.\n",
    "   A feature selection method is proposed to select a subset of variables in principal component analysis (PCA) that\n",
    "   preserves as much information present in the complete data as possible. The information is measured by means of the\n",
    "   percentage of consensus in generalised Procrustes analysis.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ffe7410-34f5-4983-9c81-42711d4c8990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Applications of PCA in Machine Learning\\n1. PCA is used to visualize multidimensional data.\\n2. It is used to reduce the number of dimensions in healthcare data.\\n3. PCA can help resize an image.\\n4. It can be used in finance to analyze stock data and forecast returns.\\n5. PCA helps to find patterns in the high-dimensional datasets.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q6\n",
    "'''Applications of PCA in Machine Learning\n",
    "1. PCA is used to visualize multidimensional data.\n",
    "2. It is used to reduce the number of dimensions in healthcare data.\n",
    "3. PCA can help resize an image.\n",
    "4. It can be used in finance to analyze stock data and forecast returns.\n",
    "5. PCA helps to find patterns in the high-dimensional datasets.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76c617d4-e889-47b4-a268-d9f69e63a623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The variance explained can be understood as the ratio of the vertical spread of the regression line\\n   (i.e., from the lowest point on the line to the highest point on the line) to the vertical spread of the data\\n   (i.e., from the lowest data point to the highest data point).\\n   PCA is defined as an orthogonal linear transformation that transforms the data to a new coordinate system such that\\n   the greatest variance by some scalar projection of the data comes to lie on the first coordinate (called the first \\n   principal component), the second greatest variance on the second coordinate, and so on.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q7\n",
    "'''The variance explained can be understood as the ratio of the vertical spread of the regression line\n",
    "   (i.e., from the lowest point on the line to the highest point on the line) to the vertical spread of the data\n",
    "   (i.e., from the lowest data point to the highest data point).\n",
    "   PCA is defined as an orthogonal linear transformation that transforms the data to a new coordinate system such that\n",
    "   the greatest variance by some scalar projection of the data comes to lie on the first coordinate (called the first \n",
    "   principal component), the second greatest variance on the second coordinate, and so on.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b8df119-e66a-4dcd-a809-ef1cfcebb3eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PCA works by finding the directions of maximum variance in the data set and projecting the data onto these directions.\\n   The principal components are ordered by the amount of variance they explain and are used for feature selection, data\\n   compression, clustering, and classification.\\n   Compute the covariance matrix for the data variables. Computing the eigenvectors and eigenvalues and order them in \\n   descending order. Then, calculate the Principal Components. Perform \\'dimensionality reduction\\' of the data set.\\n   In case of PCA, \"variance\" means summative variance or multivariate variability or overall variability or total \\n   variability. Below is the covariance matrix of some 3 variables. Their variances are on the diagonal, and the sum of \\n   the 3 values (3.448) is the overall variability.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q8\n",
    "'''PCA works by finding the directions of maximum variance in the data set and projecting the data onto these directions.\n",
    "   The principal components are ordered by the amount of variance they explain and are used for feature selection, data\n",
    "   compression, clustering, and classification.\n",
    "   Compute the covariance matrix for the data variables. Computing the eigenvectors and eigenvalues and order them in \n",
    "   descending order. Then, calculate the Principal Components. Perform 'dimensionality reduction' of the data set.\n",
    "   In case of PCA, \"variance\" means summative variance or multivariate variability or overall variability or total \n",
    "   variability. Below is the covariance matrix of some 3 variables. Their variances are on the diagonal, and the sum of \n",
    "   the 3 values (3.448) is the overall variability.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "122680ac-88a1-4380-be85-e8c7af888b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PCA generally tries to find the lower-dimensional surface to project the high-dimensional data. PCA works by \\n   considering the variance of each attribute because the high attribute shows the good split between the classes, and\\n   hence it reduces the dimensionality.\\n   Abstract: Principal component analysis (PCA) is widely used as a means of di- mension reduction for high-dimensional \\n   data analysis. A main disadvantage of the standard PCA is that the principal components are typically linear \\n   combinations of all variables, which makes the results difficult to interpret.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q9\n",
    "'''PCA generally tries to find the lower-dimensional surface to project the high-dimensional data. PCA works by \n",
    "   considering the variance of each attribute because the high attribute shows the good split between the classes, and\n",
    "   hence it reduces the dimensionality.\n",
    "   Abstract: Principal component analysis (PCA) is widely used as a means of di- mension reduction for high-dimensional \n",
    "   data analysis. A main disadvantage of the standard PCA is that the principal components are typically linear \n",
    "   combinations of all variables, which makes the results difficult to interpret.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7a8a8b-5f21-4d6c-943e-12a219f8e987",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
